{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13351,"databundleVersionId":324297,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import ParameterGrid\nimport itertools\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-03T06:15:26.741683Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Unsupervised Model - NMF","metadata":{}},{"cell_type":"markdown","source":"# Part 1: Data Loading \n\nRead the CSV files into Pandas DataFrames so we can explore and process the data.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/learn-ai-bbc/BBC News Train.csv')\ntest = pd.read_csv('/kaggle/input/learn-ai-bbc/BBC News Test.csv')\nsolution = pd.read_csv('/kaggle/input/learn-ai-bbc/BBC News Sample Solution.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 2: Exploratory Data Analysis (EDA)\n\nUnderstand what columns we have, the shape of the data, and what the labels look like. This helps guide all further steps like preprocessing and modeling.","metadata":{}},{"cell_type":"markdown","source":"**Step 2.1: Inspect the Data**\n\n* train.shape shows (number of rows, number of columns) → how many samples and what features\n* train.columns shows the names of the columns, so you know what to work with\n* train.head() shows the first 5 rows of the training data","metadata":{}},{"cell_type":"code","source":"print(\"Shape of training data:\", train.shape)\nprint(\"Columns in training data:\", train.columns)\ntrain.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 2.2: Class Distribution Analysis**\n\nUnderstand how many samples we have for each news category (label).\nIf some categories have much more data than others, this could affect how the model learns — this is called class imbalance.\n\n* train['Category'] accesses the column of labels.\n* .value_counts() counts how many times each unique label appears.","metadata":{}},{"cell_type":"code","source":"train['Category'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 2.3: Visualize the class distribution**\n\nA bar chart gives you an immediate visual of whether some categories dominate.\nIf one category appears way more often than others, your model might become biased toward it.\n\n","metadata":{}},{"cell_type":"code","source":"train['Category'].value_counts().plot(kind='bar', figsize=(8,5), title='Category Distribution')\nplt.xlabel('Category')\nplt.ylabel('Count')\nplt.grid(axis='y')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 3: Data Cleaning\n\nCheck if there are any duplicates and if they are duplicated across categories (i.e. having multiple categories) or just duplicated in a single category.\n\nThen, remove the duplicates.","metadata":{}},{"cell_type":"markdown","source":"**Step 3.1: Find Pure Duplicates (Same Text + Same Category)**\n\nFind how many rows are exact duplicates — same news text and same category.\n\n* train.duplicated() returns a boolean Series: True for rows that are duplicates of previous rows.\n* .sum() counts how many True values there are — total duplicate rows.\n\nThis helps us know if there are exact repeated examples that add no new info.","metadata":{}},{"cell_type":"code","source":"duplicate_texts = (train.duplicated(subset=[\"Text\", \"Category\"], keep=False)).sum()\nprint(f\"Total duplicate texts (same text + same category): {duplicate_texts}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 3.2: Remove Pure Duplicates**\n\nIf any were found, remove these pure duplicate rows from the DataFrame so each unique example only appears once.","metadata":{}},{"cell_type":"code","source":"train = train.drop_duplicates(subset=[\"Text\", \"Category\"])\nduplicate_texts = (train.duplicated(subset=[\"Text\", \"Category\"], keep=False)).sum()\nprint(f\"Total duplicate texts (same text + same category): {duplicate_texts}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 3.3: Find Conflict Duplicates (Same Text, Different Category)**\n\nIdentify news articles that appear more than once but with different categories, which might confuse the model.","metadata":{}},{"cell_type":"code","source":"duplicates_across_categories = train.groupby(\"Text\")[\"Category\"].nunique()\nnum_multi_category_texts = (duplicates_across_categories > 1).sum()\nprint(f\"Total texts that appear in more than one category: {num_multi_category_texts}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 4: Feature Extraction & Modeling","metadata":{}},{"cell_type":"markdown","source":"**Step 4.1: Convert text data into numerical features**\n\nText cannot be directly used by machine learning models — we need to transform it into numbers.\n\nWe’ll start with a commonly used method: TF-IDF (Term Frequency–Inverse Document Frequency).\n\n* TF tells how often a word appears in a document.\n\n* IDF down-weights common words (like \"the\", \"is\", \"and\") that are less useful for classification.\n\nIt gives us a sparse numerical matrix, which we can feed into ML models.","metadata":{}},{"cell_type":"markdown","source":"parameters:\n* stop_words='english' filters common english words\n* max_df=0.95 sets the threshhold to if 95% of documents have this word, ignore it\n* min_df sets the threshhold of if 2 or more rows have this word , ignore it","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\nX_train = vectorizer.fit_transform(train['Text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 4.2: Building and Training Models**\n\n","metadata":{}},{"cell_type":"markdown","source":"NMF is commonly used for topic modeling. It decomposes the document-word matrix into:\n\nDocument-topic matrix (how strongly each doc belongs to each topic)\n\nTopic-word matrix (what words are important in each topic)\n\n* NMF = Non-negative Matrix Factorization.\n\n* n_components=5: tells NMF to find 5 topics/clusters. We choose 5 because the BBC dataset has 5 categories.\n\n* .fit(X_train): learns the topics (hidden patterns) from the TF-IDF matrix.","metadata":{}},{"cell_type":"code","source":"model = NMF(n_components=5).fit(X_train)\ny_pred_train = model.transform(X_train)\ny_pred_train_as_int = y_pred_train.argmax(axis=1)\n\nprint(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def label_permute_compare(ytdf,yp,n=5):\n    \"\"\"\n    ytdf: labels dataframe object\n    yp: clustering label prediction output\n    Returns permuted label order and accuracy. \n    Example output: (3, 4, 1, 2, 0), 0.74 \n    \"\"\"\n    label_encoder = LabelEncoder()\n    y_true = label_encoder.fit_transform(ytdf.values.ravel())\n    \n    yp = np.array(yp)\n    \n    best_acc = 0\n    best_perm = None\n    \n    for perm in itertools.permutations(range(n)):\n        mapped_yp = [perm[label] for label in yp]\n        acc = accuracy_score(y_true, mapped_yp)\n        if acc > best_acc:\n            best_acc = acc\n            best_perm = perm\n    return best_perm, best_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Compare Predictions to True Labels","metadata":{}},{"cell_type":"code","source":"perm, acc = label_permute_compare(train[['Category']], y_pred_train_as_int, n=5)\n\nprint(\"Accuracy for the train data: \", acc)\nprint(\"Best permutation:\", perm)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display the Confusion Matrix for the train data ","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ny_true_train = label_encoder.fit_transform(train['Category'].values.ravel())\n\npred_map = [perm[label] for label in y_pred_train_as_int]\n\ncm = confusion_matrix(y_true_train, pred_map)\ndisplay = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\nfig, ax = plt.subplots(figsize=(8, 6))\ndisplay.plot(xticks_rotation=45, cmap=\"Reds\", ax=ax)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 5: Use the Trained Model to Predict the Test Data","metadata":{}},{"cell_type":"markdown","source":"**Step 5.1: Transform the Test Data**","metadata":{}},{"cell_type":"code","source":"X_test = vectorizer.transform(test['Text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 5.2: Use NMF Model to Predict Topics on Test Data**","metadata":{}},{"cell_type":"code","source":"y_pred_test = model.transform(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 5.3: Pick the Most Likely Topic per Article**","metadata":{}},{"cell_type":"code","source":"y_pred_test_as_int = y_pred_test.argmax(axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 5.4: Map NMF Topic Numbers to Actual Category Names**","metadata":{}},{"cell_type":"code","source":"mapped_preds_test = [perm[label] for label in y_pred_test_as_int]\npredicted_labels_test = label_encoder.classes_[mapped_preds_test]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create the Submission File","metadata":{}},{"cell_type":"code","source":"#submission = pd.DataFrame({\n#    'ArticleId': test['ArticleId'],\n#    'Category': predicted_labels_test\n#})\n#submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Unsupervised Model - NMF accuracy score: 0.92380","metadata":{}},{"cell_type":"markdown","source":"# Part 6: Changing Hyperparameter","metadata":{}},{"cell_type":"markdown","source":"We can use ParameterGrid to explore different parameter combinations and identify those that yield the highest training accuracy. From the results, we’ll select the top 5 configurations and evaluate their performance on the test set by manually submitting the predictions to see if they lead to improved test accuracy.","metadata":{}},{"cell_type":"code","source":"results = []\n\nparam_grid = {\n    'init': ['random', 'nndsvd'],\n    'solver': ['cd', 'mu'],\n    'beta_loss': ['frobenius', 'kullback-leibler'],\n    'alpha': [0, 0.05, 0.1],\n}\n\nfor params in ParameterGrid(param_grid):\n    \n    model_grid = NMF(n_components=5, init=params['init'], solver=params['solver'], alpha_W=params['alpha'], alpha_H=params['alpha'], max_iter=500).fit(X_train)\n    y_pred_grid = model_grid.transform(X_train)\n    y_pred_as_int_grid = y_pred_grid.argmax(axis=1)\n\n    perm_grid, acc_grid = label_permute_compare(train[['Category']], y_pred_as_int_grid, n=5)\n\n    results.append({\n        'init': params['init'],\n        'solver': params['solver'],\n        'beta_loss': params['beta_loss'],\n        'accuracy': acc_grid,\n        'test accuracy': None,\n        'perm': perm_grid,\n        'model': model_grid\n    })\n\nresults_df = pd.DataFrame(results)\n\n# Display the table\nresults_df.sort_values(by='accuracy', ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_df.loc[2, 'test accuracy'] = 0.92230\nresults_df.loc[6, 'test accuracy'] = 0.92230\nresults_df.loc[5, 'test accuracy'] = 0.90384\nresults_df.loc[0, 'test accuracy'] = 0.91092\nresults_df.loc[1, 'test accuracy'] = 0.67029\n\nresults_df.sort_values(by='test accuracy', ascending=False).head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Based on the table above, the best is nndsvd, with cd, alpha_H 0, and alpha_W 0 with a test score of 0.9223. \n\nThe result is not better than the original setting accuract score 0.92380.","metadata":{}},{"cell_type":"markdown","source":"# Supervised Model - Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train['Category'])\n\n# Train logistic regression\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Predict on test data\ny_test_pred = model.predict(X_test)\npredicted_labels = label_encoder.inverse_transform(y_test_pred)\n\n# Create submission file\n# submission = pd.DataFrame({\n#    'ArticleId': test['ArticleId'],\n#    'Category': predicted_labels\n#})\n\n#submission.to_csv('logreg_submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Supervised Model - Logistic Regression accuracy score: 0.98231","metadata":{}},{"cell_type":"code","source":"# os.remove(\"/kaggle/working/svm_submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train['Category'])\n\n# Train SVM classifier\nmodel = SVC(kernel='linear', probability=True)  \nmodel.fit(X_train, y_train)\n\n# Predict on test data\ny_test_pred = model.predict(X_test)\n\n# Convert numeric predictions back to original category labels\npredicted_labels = label_encoder.inverse_transform(y_test_pred)\n\n# Create submission DataFrame\n#submission = pd.DataFrame({\n#    'ArticleId': test['ArticleId'],\n#    'Category': predicted_labels\n#})\n\n# Save to CSV\n#submission.to_csv('svm_submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T06:17:57.128388Z","iopub.execute_input":"2025-06-03T06:17:57.128743Z","iopub.status.idle":"2025-06-03T06:18:19.056252Z","shell.execute_reply.started":"2025-06-03T06:17:57.128708Z","shell.execute_reply":"2025-06-03T06:18:19.055049Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"Supervised Model - SVM classifier accuracy score: 0.98095","metadata":{}},{"cell_type":"markdown","source":"Finally, to evaluate how supervised learning algorithms perform when the training data is randomly split. We'll use train_test_split to divide the training set based on a specified fraction. Each model will be trained on this split subset and then used to make predictions on the full original training set to assess accuracy.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfor frac in [0.1, 0.2, 0.5, 0.9]:\n    X_train_frac, _, y_train_frac, _ = train_test_split(X_train, y_train, train_size=frac)\n\n    model_log = LogisticRegression(max_iter=1000)\n    model_log.fit(X_train_frac, y_train_frac)\n    y_pred_log = model_log.predict(X_train)\n    perm_log, acc_log = label_permute_compare(train[['Category']], y_pred_log, n=5)\n    print(f\"LogReg ({int(frac*100)}% train): Accuracy = {acc_log:.3f}\")\n\n    model_svm = SVC()\n    model_svm.fit(X_train_frac, y_train_frac)\n    y_pred_svm = model_svm.predict(X_train)\n    perm_svm, acc_svm = label_permute_compare(train[['Category']], y_pred_svm, n=5)\n    print(f\"SVM    ({int(frac*100)}% train): Accuracy = {acc_svm:.3f}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T06:18:55.598066Z","iopub.execute_input":"2025-06-03T06:18:55.598978Z","iopub.status.idle":"2025-06-03T06:19:09.390198Z","shell.execute_reply.started":"2025-06-03T06:18:55.598943Z","shell.execute_reply":"2025-06-03T06:19:09.389277Z"}},"outputs":[{"name":"stdout","text":"LogReg (10% train): Accuracy = 0.777\nSVM    (10% train): Accuracy = 0.679\nLogReg (20% train): Accuracy = 0.894\nSVM    (20% train): Accuracy = 0.860\nLogReg (50% train): Accuracy = 0.978\nSVM    (50% train): Accuracy = 0.974\nLogReg (90% train): Accuracy = 0.994\nSVM    (90% train): Accuracy = 0.996\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"We observe that using smaller subsets of data generally results in lower accuracy. However, SVM maintains consistently high accuracy across different data proportions. In contrast, Logistic Regression needs at least 20% of the data to achieve accuracy in the high 0.9x range. This suggests that supervised learning models, particularly SVM, have the potential to perform well even with reduced training data.","metadata":{}},{"cell_type":"markdown","source":"# Summary: Comparison between Unsupervised and Supervised model\n\n","metadata":{}},{"cell_type":"markdown","source":"**Ranking from highest accuracy score:**\n\n1. Supervised Model - Logistic Regression accuracy score: 0.98231\n2. Supervised Model - SVM classifier accuracy score: 0.98095\n3. Unsupervised Model - NMF accuracy score: 0.92380\n\n**Conclusion:**\n\nThe supervised models, Logistic Regression and SVM, both achieved very high accuracy scores above 98%, demonstrating their strong performance in classifying the labeled data. Logistic Regression slightly outperformed SVM by a small margin. In contrast, the unsupervised model, NMF, showed a lower accuracy around 92%, which is expected since it does not use label information during training. \n\nOverall, these results highlight the advantage of supervised learning methods when labeled data is available, while unsupervised methods like NMF can still provide useful, though less accurate, topic grouping without supervision.","metadata":{}}]}